# Session 4.1 {#session-4-1}

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(patchwork)
library(performance)
library(correlation)
library(psych)
library(tidyverse)
mh <- read_csv("data/reading_iq.csv")
```


In this chapter we will use a simulated version of a dataset adapted from [Miller and Haden (2013)](https://drive.google.com/file/d/0B1fyuTuvj3YoaFdUR3FZaXNuNXc/view){target="_blank"}, Chapter 11, looking at the relationship between four variables: reading ability, intelligence (IQ), the number of minutes per week spent reading at home (Home); and the number of minutes per week spent watching TV at home (TV). 

## Set-up

Open your Workshop project and do the following:

* Create and save a new R Markdown document named Session 4.1. get rid of the default template text from line 11 onwards.
* Load the required packages and data as below.

```{r eval = FALSE}
library(patchwork)
library(correlation)
library(psych)
library(performance)
library(tidyverse)
mh <- read_csv("https://raw.githubusercontent.com/PsyTeachR/quant-fun-v2/main/book/data/chpt9/MillerHadenData.csv")
```


## Look at the data {#corr-a2}

If you have loaded in the data correctly then you should be able to have a look at it through one of the various methods we have looked at already.

* Look at your data using the `head()` function and you should see the following:

```{r chp9-show-head, echo=FALSE}
head(mh)
```

As you can see, we have five columns and they are: 

* the participant number (`Participant`), 
* Reading Ability score (`Abil`), 
* Intelligence score (`IQ`), 
* the number of minutes spent reading at Home per week (`Home`), 
* and the number of minutes spent watching TV per week (`TV`). 

Here we will we will focus on Reading Ability and IQ but for further practice you can look at other relationships in the optional exercises.  

A probable `r glossary("hypothesis")` is that as Intelligence predicts Reading Ability and we'll test this by performing a correlation then linear regression 

## Descriptive statistics and visualisation

First, we'll computer descriptive statistics for our variables of interest using `summarise()` as we've done previously.

```{r descriptives, echo = TRUE}

descriptives <- summarise(mh, 
                          Abil_mean = mean(Abil),
                          Abil_SD = sd(Abil),
                          IQ_mean = mean(IQ),
                          IQ_SD = sd(IQ))

descriptives
```

We can also then plot the relationship between the two variables on a scatterplot using `ggplot()`. We'll use `geom_jiter()` rather than `geom_point()` as there as a fe overlaping scores.

```{r}
ggplot(mh, aes(x = IQ, y = Abil)) +
  geom_jitter() +
  geom_smooth(method = "lm")
```


## Correlation

There's actually a lot of different packages and functions that you can use to run correlations, we'll use the `correlation()` function from the `correlation` package. Remember that for help on any function you can type e.g., `?correlation` in the console window.  The `correlation()` function requires:

* The name of the data set you are using
* The name of the first variable you want to select for the correlation
* The name of the second variable you want to select for the correlation
* The type of correlation you want to run: e.g. `pearson`, `spearman`
* The type of NHST tail you want to run: e.g. `"less"`,`"greater"`, `"two.sided"`

In our case, we want to correlate IQ and Abil in the dataset `mh`, we're going to use a Pearson correlation, and because we have a directional hypothesis (IQ and Abil should be positively correlated), we'll use a one-sided test to increase power.

```{r cor_example, eval = FALSE}
correlation(data = mh, 
            select = "IQ", 
            select2 = "Abil",  
            method = "pearson", 
            alternative = "greater")
```

## Multiple Correlations

We're only going to focus on the relationship between two variables but if you want to conduct correlations between multiple variables in data set it's easy to do with the `correlation` package. 

The `pairs.panels())` function comes from the `psych` library and creates a matrix of scatterplots, with the histograms, and correlation coefficients which you can then use to give you an overview of all the relationships at the one time. So it is useful for checking assumptions in one place.

The code below:

* Takes the dataset `mh` and then;
* Uses `select()` to get rid of the `Participant` column and then;
* Pipes the remaining data into the `pairs.panels()` function
* The additional arguments:
  * `ellipses = FALSE` turns off the [correlation ellipses](https://analyse-it.com/docs/user-guide/multivariate/scatter-plot#:~:text=If%20the%20association%20is%20a,more%20the%20variables%20are%20uncorrelated.){target="_blank"}, 
  * `lm = TRUE` use a linear line of best fit, 
  * `method = "pearson", specifies a Pearson correlation.

There are additional arguments to adjust the plot `pairs.panel` creates that you can look up in the help documentation if you are interested.

```{r}
mh %>%
  select(-Participant) %>%
  pairs.panels(ellipses = FALSE, 
               lm = TRUE, 
               method = "pearson")
```

To perform multiple correlations in one go, we will again use the `correlation()` function but rather than specifying two variables to correlate, you can also provide a data frame that has multiple variables  and it will run all possible correlations between the variables. Similar to above, we want to remove the `Participant` column before we do this.   

* `method` controls which correlation is computed, the default is `pearson` but if you needed to run the non-parametric version you could change this to `spearman`.  
* `p_adjust` allows you to apply a correction for multiple comparisons to your correlation analysis.  
* Because you're running multiple correlations and some may be positive and some may be negative, there is no option to specify a one or two-tailed test.   

```{r corrresults}
corr_results <- mh %>%
  select(-Participant) %>%
  correlation(method = "pearson", 
              p_adjust = "bonferroni")

corr_results
```

## Linear regression

To perform a linear regression is very simple using the function `lm()` (linear model). First you construct the model, then you use `summary()` to summarise the results.

* `~` is a tilde and in this context is probably most usefully translated as "by", i.e., predict Ability by IQ using the data in mh.

```{r}
mod <- lm(formula = Abil ~ IQ, data = mh)
summary(mod)
```

## Assumptions

Finally, you can also check that your model meets the assumptions of linear regression using the `performance` package. This really is an excellent tool as it contains (amongst may other things), a handy wrapper function `check_model()` that performs all your assumption tests and provides clear and interpretable output:

```{r}
check_model(mod)
```

**If you get the error `Error in grid.Call(C_convert, x, as.integer(whatfrom), as.integer(whatto),  : Viewport has zero dimension(s)`, increase the size of the plot pane in RStudio (literally drag and make it bigger). **

## Multiple regression

If you want to add in additional predictors, you can do so by adjusting the formula:

```{r}
mod2 <- lm(formula = Abil ~ IQ + Home, data = mh) # no interaction
summary(mod2)

mod3 <- lm(formula = Abil ~ IQ * Home, data = mh) # with interaction between predictors
summary(mod3)
```

## Glossary {#glossary-regression}

`r glossary_table()`

## Further resources {#resources-regression}

* [Learning Statistical Models Through Simulation in R](https://psyteachr.github.io/stat-models-v1/)
* [Introduction to GLM](https://psyteachr.github.io/reprores-v2/glm.html)

